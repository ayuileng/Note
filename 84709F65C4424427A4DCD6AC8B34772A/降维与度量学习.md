### 降维与度量学习

## PCA
* 主成分分析(Principal Component Analysis)是一种常用的数据分析方法。通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。
* 向量内积的含义：A与B的内积等于A到B的投影长度乘以B的模。
* 要准确描述向量，首先要确定一组基(d个线性无关的向量，其中d为向量空间的维数)，然后给出在基所在的各个直线上的投影值。
* 任何d个线性无关的d维向量都可以成为一组基，一般来说希望基是标准基(不一定非得要正交)，因为如果基的模是1的话，可以方便的用向量点乘基而直接获得其在新基上的坐标了。
* 基变换的矩阵表示：推广一下，如果我们有m个d维向量，只要将d维向量按列排成一个d行m列矩阵，然后用“基矩阵”乘以这个矩阵，就得到了所有这些向量在新基下的值。数学表示为：
```math
\left\{
 \begin{matrix}
   p_1 \\
   p_2 \\
   ... \\
   p_R \\
  \end{matrix}
  \right\}(a_1,a_2,...,a_M)=
\left\{
 \begin{matrix}
   p_1a_1 & p_1a_2 & ... & p_1a_M \\
   p_2a_1 & p_2a_2 & ... & p_2a_M \\
   ... & ... & ... & ... \\
   p_Ra_1 & p_Ra_2 & ... & p_Ra_M \\
\end{matrix}
\right\}
```
* 矩阵相乘的物理意义：**两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。**
* 如果基的数量少于向量本身的维数，则可以达到降维的效果。**如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息**？
* 例：如果将二维空间上的数据降维至一维，则表示在平面上选择一条直线，是这些原来的二维空间上的点投影到这条直线，使它们的投影值尽量分散，即**方差最大**。
* 如果是多维数据的话，通过方差最大能选出第一条坐标轴，然后之后的坐标轴该如何选取呢？应该保证第二条坐标轴和第一条坐标轴尽量`不相`，衡量独立的参数是**协方差**，协方差等于0时表示这两个坐标轴不相关.
* 当对样本进行中心化(都减去平均值，保证样本的和为0)后，协方差的表达式为：
```math
Cov(a,b)=\frac{1}{m} \sum _{i=1}^m a_ib_i
```
    要使协方差=0，必须保证两个向量正交，因此最后选择出来的所有坐标轴肯定是正交的。
* 于是，问题的整体描述为：**将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。**
* 设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设`$C=\frac{1}{m}XX^T$`，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差
* 于是目标就是将协方差矩阵对角化：对角线以外的元素全为0，并且在主对角线上的元素进行排序，取最大的k个。
